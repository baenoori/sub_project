from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.chat_models.openai import ChatOpenAI
import os
import streamlit as st
from langdetect import detect
import openai
st.set_page_config(page_title="ë‹¤ì¤‘ ì–¸ì–´ ì§€ì› Q&A ì„œë¹„ìŠ¤", page_icon='ğŸ’¬')

st.title('ğŸ’¬ ë‹¤ì¤‘ ì–¸ì–´ ì§€ì› Q&A ì„œë¹„ìŠ¤')

# PDF íŒŒì¼ ë¡œë“œ
def load_pdf(uploaded_file):
    # Streamlit íŒŒì¼ ê°ì²´ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_file_path = "./temp_uploaded_file.pdf"
    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    # PyMuPDFLoaderë¡œ ë¡œë“œ
    loader = PyMuPDFLoader(temp_file_path)
    documents = loader.load()
    return documents

# í…ìŠ¤íŠ¸ ë¶„í• 
def split_text(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)
    return texts

# ê¸°ì¡´ VectorDB ë¡œë“œ ë° ìƒˆë¡œìš´ ë°ì´í„° ë³‘í•©
def update_vectorstore(new_texts):
    persist_directory = "./ë‹¤ì¤‘ì–¸ì–´ì§€ì›_QNA_ì„œë¹„ìŠ¤/chroma_db"
    embeddings = OpenAIEmbeddings()

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(persist_directory):
        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        print("ê¸°ì¡´ VectorDB ë¡œë“œ ì™„ë£Œ.")
    else:
        vectorstore = Chroma(embedding_function=embeddings, persist_directory=persist_directory)
        print("ìƒˆë¡œìš´ VectorDB ìƒì„±.")

    # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
    vectorstore.add_documents(new_texts)
    print("ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ.")

    # VectorDB ì €ì¥
    vectorstore.persist()
    print("VectorDB ì €ì¥ ì™„ë£Œ.")
    return vectorstore

# ì±—ë´‡ ìƒì„±
def create_chatbot(vectorstore):
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    retriever = vectorstore.as_retriever()
    qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever)
    return qa_chain

def translate_some_to_en(text, input_language):
    openai.api_key = os.getenv("OPENAI_API_KEY")
    prompt = f"Translate the following text to english:\n{text}"
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful translation assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000
        )
        translation = response['choices'][0]['message']['content'].strip()
        return translation
    except Exception as e:
        st.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def translate_en_to_some(text, input_language):
    openai.api_key = os.getenv("OPENAI_API_KEY")
    prompt = f"Translate the following text to {input_language}:\n{text}"
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful translation assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000
        )
        translation = response['choices'][0]['message']['content'].strip()
        return translation
    except Exception as e:
        st.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    language_map = {
        "af": "Afrikaans", "ar": "Arabic", "bg": "Bulgarian", "bn": "Bengali",
        "ca": "Catalan", "cs": "Czech", "cy": "Welsh", "da": "Danish",
        "de": "German", "el": "Greek", "en": "English", "es": "Spanish",
        "et": "Estonian", "fa": "Persian", "fi": "Finnish", "fr": "French",
        "gu": "Gujarati", "he": "Hebrew", "hi": "Hindi", "hr": "Croatian",
        "hu": "Hungarian", "id": "Indonesian", "it": "Italian", "ja": "Japanese",
        "kn": "Kannada", "ko": "Korean", "lt": "Lithuanian", "lv": "Latvian",
        "mk": "Macedonian", "ml": "Malayalam", "mr": "Marathi", "ne": "Nepali",
        "nl": "Dutch", "no": "Norwegian", "pa": "Punjabi", "pl": "Polish",
        "pt": "Portuguese", "ro": "Romanian", "ru": "Russian", "sk": "Slovak",
        "sl": "Slovenian", "so": "Somali", "sq": "Albanian", "sv": "Swedish",
        "sw": "Swahili", "ta": "Tamil", "te": "Telugu", "th": "Thai",
        "tl": "Tagalog", "tr": "Turkish", "uk": "Ukrainian", "ur": "Urdu",
        "vi": "Vietnamese", "zh-cn": "Chinese (Simplified)", "zh-tw": "Chinese (Traditional)"
    }
    
    # PDF ì—…ë¡œë“œ
    pdf_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])
    if pdf_file:
        # PDF ë¡œë“œ ë° ì²˜ë¦¬
        documents = load_pdf(pdf_file)
        texts = split_text(documents)
        vectorstore = update_vectorstore(texts)
    else:
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma(persist_directory="./ë‹¤ì¤‘ì–¸ì–´ì§€ì›_QNA_ì„œë¹„ìŠ¤/chroma_db", embedding_function=embeddings)

    # ì±—ë´‡ ìƒì„±
    chatbot = create_chatbot(vectorstore)
    chat_history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥

    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.text_input("ì‚¬ìš©ì ì§ˆë¬¸:")
    
    if st.button("ì§ˆë¬¸í•˜ê¸°") and user_input:
        # ì–¸ì–´ ê°ì§€ ë° ì–¸ì–´ ì½”ë“œ í™•ì¸
        language_code = detect(user_input)
        input_language_name = language_map.get(language_code, "ì•Œ ìˆ˜ ì—†ëŠ” ì–¸ì–´")
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì˜ì–´ë¡œ ë²ˆì—­
        translated_query = translate_some_to_en(user_input, input_language_name)
        response = chatbot({"question": translated_query, "chat_history": chat_history})
        
        # ì˜ì–´ ë‹µë³€ì„ ì›ë˜ ì–¸ì–´ë¡œ ë²ˆì—­
        translated_response = translate_en_to_some(response['answer'], input_language_name).replace("<|eot_id|>", "")
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        chat_history.append({"role": "user", "content": user_input})
        chat_history.append({"role": "assistant", "content": translated_response})
        
        # ì¶œë ¥
        st.write("ğŸ’¬ ì§ˆë¬¸:", user_input)
        st.write("ğŸ¤– ë‹µë³€:", translated_response)

if __name__ == "__main__":
    main()